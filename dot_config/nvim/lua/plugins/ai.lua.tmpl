local kind_icons = {
    -- LLM Provider icons
    claude = '󰋦',
    openai = '󱢆',
    codestral = '󱎥',
    gemini = '',
    Groq = '',
    Openrouter = '󱂇',
    Ollama = '󰳆',
    ['Llama.cpp'] = '󰳆',
    Deepseek = ''
}
local source_icons = {
    minuet = '󱗻',
    orgmode = '',
    otter = '󰼁',
    nvim_lsp = '',
    lsp = '',
    buffer = '',
    luasnip = '',
    snippets = '',
    path = '',
    git = '',
    tags = '',
    cmdline = '󰘳',
    latex_symbols = '',
    cmp_nvim_r = '󰟔',
    codeium = '󰩂',
    -- FALLBACK
    fallback = '󰜚',
}
return {

{{- if ne .location "client" -}}
  {
    "yetone/avante.nvim",
    -- if you want to build from source then do `make BUILD_FROM_SOURCE=true`
    -- ⚠️ must add this setting! ! !
    build = "make",
    event = "VeryLazy",
    version = false, -- Never set this value to "*"! Never!
    opts = {
      input = {
        provider = "snacks",
        provider_opts = {
          -- Additional snacks.input options
          title = "Avante Input",
          icon = " ",
        },
      },
{{- if eq .location "client" -}}
      provider = "copilot",
{{- else -}}
      provider = "ollama",
      -- provider = "gemini",
{{ end }}
      providers = {
          ollama = {
            endpoint = "http://localhost:11434",
            model = "qwen2.5-coder",
          },
      },
      {
        -- system_prompt as function ensures LLM always has latest MCP server state
        -- This is evaluated for every message, even in existing chats
        system_prompt = function()
            local hub = require("mcphub").get_hub_instance()
            return hub and hub:get_active_servers_prompt() or ""
        end,
        -- Using function prevents requiring mcphub before it's loaded
{{- if ne .location "client" -}}
        custom_tools = function()
            return {
                require("mcphub.extensions.avante").mcp_tool(),
            }
        end,
{{ end }}
      }
    },
    dependencies = {
      "nvim-lua/plenary.nvim",
      "MunifTanjim/nui.nvim",
      --- The below dependencies are optional,
      "echasnovski/mini.pick", -- for file_selector provider mini.pick
      "nvim-telescope/telescope.nvim", -- for file_selector provider telescope
      "hrsh7th/nvim-cmp", -- autocompletion for avante commands and mentions
      "ibhagwan/fzf-lua", -- for file_selector provider fzf
      "stevearc/dressing.nvim", -- for input provider dressing
      "folke/snacks.nvim", -- for input provider snacks
      "nvim-mini/mini.icons", -- or nvim-tree/nvim-web-devicons
      {
        -- Make sure to set this up properly if you have lazy=true
        'MeanderingProgrammer/render-markdown.nvim',
        opts = {
          file_types = { "markdown", "Avante" },
        },
        ft = { "markdown", "Avante" },
      },
    },
  },
{{- end -}}
{{- if eq .location "client" -}}
  {
      "CopilotC-Nvim/CopilotChat.nvim",
      opts = function()
        local user = vim.env.USER or "User"
        user = user:sub(1, 1):upper() .. user:sub(2)
        return {
          auto_insert_mode = true,
          question_header = "  " .. user .. " ",
          answer_header = "  Copilot ",
          window = {
            width = 0.4,
          },
        }
      end,
      keys = {
      { "<c-s>", "<CR>", ft = "copilot-chat", desc = "Submit Prompt", remap = true },
      { "<leader>a", "", desc = "+ai", mode = { "n", "v" } },
      {
        "<leader>aa",
        function()
          return require("CopilotChat").toggle()
        end,
        desc = "Toggle (CopilotChat)",
        mode = { "n", "v" },
      },
      {
        "<leader>ax",
        function()
          return require("CopilotChat").reset()
        end,
        desc = "Clear (CopilotChat)",
        mode = { "n", "v" },
      },
      {
        "<leader>aq",
        function()
          vim.ui.input({
            prompt = "Quick Chat: ",
          }, function(input)
            if input ~= "" then
              require("CopilotChat").ask(input)
            end
          end)
        end,
        desc = "Quick Chat (CopilotChat)",
        mode = { "n", "v" },
      },
      {
        "<leader>ap",
        function()
          require("CopilotChat").select_prompt()
        end,
        desc = "Prompt Actions (CopilotChat)",
        mode = { "n", "v" },
      },
    },
  },
  {
    "zbirenbaum/copilot.lua",
    cmd = "Copilot",
    build = ":Copilot auth",
    event = "BufReadPost",
    opts = {
      suggestion = {
        -- enabled = not vim.g.ai_cmp,
        enabled = false,
        auto_trigger = true,
        -- hide_during_completion = vim.g.ai_cmp,
        hide_during_completion = true,
        keymap = {
          accept = false, -- handled by nvim-cmp / blink.cmp
          next = "<M-]>",
          prev = "<M-[>",
        },
      },
      panel = { enabled = false },
      filetypes = {
        markdown = true,
        javascript = true,
        typescript = true,
        typescriptreact = true,
        javascriptreact = true,
        help = true,
      },
    },
  },
  {
    "zbirenbaum/copilot.lua",
    opts = function()
      LazyVim.cmp.actions.ai_accept = function()
        if require("copilot.suggestion").is_visible() then
          LazyVim.create_undo()
          require("copilot.suggestion").accept()
          return true
        end
      end
    end,
  },
  {
    "giuxtaposition/blink-cmp-copilot",
  },
{{- else -}}
  {
    "milanglacier/minuet-ai.nvim",
    config = function()
      require('blink-cmp').setup {
        appearance = {
            use_nvim_cmp_as_default = true,
            nerd_font_variant = 'normal',
            kind_icons = kind_icons
        },
        completion = {
          menu = {
              draw = {
                  columns = {
                      { 'label', 'label_description', gap = 1 },
                      { 'kind_icon', 'kind' },
                      { 'source_icon' },
                  },
                  components = {
                      source_icon = {
                          -- don't truncate source_icon
                          ellipsis = false,
                          text = function(ctx)
                              return source_icons[ctx.source_name:lower()] or source_icons.fallback
                          end,
                          highlight = 'BlinkCmpSource',
                      },
                  },
              },
          },
        },
      }
      require("minuet").setup({
        provider = "openai_fim_compatible",
        -- provider = "gemini",
        n_completions = 1, -- recommend for local model for resource saving
        add_single_line_entry = false,
        -- I recommend beginning with a small context window size and incrementally
        -- expanding it, depending on your local computing power. A context window
        -- of 512, serves as an good starting point to estimate your computing
        -- power. Once you have a reliable estimate of your local computing power,
        -- you should adjust the context window to a larger value.
        context_window = 512,
        provider_options = {
          openai_fim_compatible = {
            api_key = "TERM",
            name = "Ollama",
            end_point = "http://localhost:11434/v1/completions",
            model = "qwen2.5-coder",
            optional = {
              max_tokens = 56,
              top_p = 0.9,
            },
          },
          gemini = {
            end_point = 'https://generativelanguage.googleapis.com/v1beta/models',
            optional = {
              generationConfig = {
                maxOutputTokens = 256,
                -- When using `gemini-2.5-flash`, it is recommended to entirely
                -- disable thinking for faster completion retrieval.
                thinkingConfig = {
                    thinkingBudget = 0,
                },
            },
            safetySettings = {
                {
                    -- HARM_CATEGORY_HATE_SPEECH,
                    -- HARM_CATEGORY_HARASSMENT
                    -- HARM_CATEGORY_SEXUALLY_EXPLICIT
                    category = 'HARM_CATEGORY_DANGEROUS_CONTENT',
                    -- BLOCK_NONE
                    threshold = 'BLOCK_ONLY_HIGH',
                },
            },
            },
          },
        },
      })
    end,
  },
{{- end -}}
  {
    "saghen/blink.cmp",
    optional = true,
    dependencies = { 
{{- if eq .location "client" -}}
      "giuxtaposition/blink-cmp-copilot",
{{- else -}}
      "milanglacier/minuet-ai.nvim",
{{- end -}}
      "Kaiser-Yang/blink-cmp-avante",
    },
    opts = {
      appearance = {
        use_nvim_cmp_as_default = true,
        nerd_font_variant = 'normal',
        kind_icons = kind_icons
      },
      completion = {
        menu = {
          draw = {
            columns = {
              { 'label', 'label_description', gap = 1 },
              { 'kind_icon', 'kind' },
              { 'source_icon' },
            },
            components = {
              source_icon = {
                -- don't truncate source_icon
                ellipsis = false,
                text = function(ctx)
                  return source_icons[ctx.source_name:lower()] or source_icons.fallback
                end,
                highlight = 'BlinkCmpSource',
              },
            },
          },
        },
      },
      sources = {
{{- if eq .location "client" -}}
        default = { "avante", "copilot" },
{{- else -}}
        default = { "avante", "minuet" },
{{- end }}
        providers = {
          copilot = {
            name = "copilot",
            module = "blink-cmp-copilot",
            kind = "Copilot",
            score_offset = 100,
            async = true,
          },
          minuet = { 
            name = "minuet",
            module = "minuet.blink",
            score_offset = 100,
          },
          avante = {
              module = 'blink-cmp-avante',
              name = 'Avante',
              opts = { }
          },
        },
      },
    },
  },
  {
    "ravitemer/mcphub.nvim",
    dependencies = {
        "nvim-lua/plenary.nvim",
    },
    build = "npm install -g mcp-hub@latest",  -- Installs `mcp-hub` node binary globally
    config = function()

{{- if eq .location "client" -}}
{{- else -}}
      require("mcphub").setup({
        extensions = {
          copilotchat = {
              enabled = true,
              convert_tools_to_functions = true,     -- Convert MCP tools to CopilotChat functions
              convert_resources_to_functions = true, -- Convert MCP resources to CopilotChat functions  
              add_mcp_prefix = false,                -- Add "mcp_" prefix to function names
          }
        }
      })
{{- end -}}
        require("mcphub").setup({
        extensions = {
            avante = {
                make_slash_commands = true, -- make /slash commands from MCP server prompts
            }
        }
      })
    end
  },
}
